{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:11.863123Z",
     "start_time": "2018-03-27T06:57:11.747195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "\n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    # 训练集中数量最少一类的狗的数量。\n",
    "    min_num_train_per_label = (\n",
    "        Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # 验证集中每类狗的数量。\n",
    "    num_valid_per_label = math.floor(min_num_train_per_label * valid_ratio)\n",
    "    label_count = dict()\n",
    "\n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "    # 整理训练和验证集。\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < num_valid_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "\n",
    "    # 整理测试集。\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:22.633745Z",
     "start_time": "2018-03-27T06:57:11.864938Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/yansheng/kaggle_dog'\n",
    "train_dir = 'train'\n",
    "label_file = 'labels.csv'\n",
    "test_dir = 'test'\n",
    "input_dir = 'train_valid_test'\n",
    "\n",
    "batch_size = 128\n",
    "valid_ratio = 0.1\n",
    "reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:43.695274Z",
     "start_time": "2018-03-27T06:57:42.560575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 224, 224)\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 224, 224), resize=0, \n",
    "                        rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                        mean=np.array([0.485, 0.456, 0.406]), std=np.array([0.229, 0.224, 0.225]), \n",
    "                        brightness=0, contrast=0, \n",
    "                        saturation=0, hue=0, \n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 224, 224)\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 224, 224), \n",
    "                        mean=np.array([0.485, 0.456, 0.406]), \n",
    "                        std=np.array([0.229, 0.224, 0.225]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:43.869093Z",
     "start_time": "2018-03-27T06:57:43.697143Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/yansheng/kaggle_dog'\n",
    "train_dir = 'train'\n",
    "label_file = 'labels.csv'\n",
    "test_dir = 'test'\n",
    "input_dir = 'train_valid_test'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "input_str = data_dir + '/' + input_dir + '/'\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1,\n",
    "                                     transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1,\n",
    "                                     transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid',\n",
    "                                           flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1,\n",
    "                                     transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True,\n",
    "                          last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "\n",
    "# 交叉熵损失函数。\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:43.892135Z",
     "start_time": "2018-03-27T06:57:43.870727Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "def Classifier(num_out):\n",
    "    net = nn.HybridSequential()\n",
    "    net.add(nn.Dense(512, activation='relu'))\n",
    "    net.add(nn.Dropout(.5))\n",
    "    net.add(nn.Dense(num_out))\n",
    "    return net\n",
    "\n",
    "def get_net(ctx):\n",
    "    pretrained_model = gluon.model_zoo.vision.resnet50_v2(pretrained=True)\n",
    "    feature_net = pretrained_model.features\n",
    "    feature_net.collect_params().setattr('grad_req', 'null')\n",
    "    num_outputs = 120\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(feature_net)\n",
    "        net.add(Classifier(num_outputs))\n",
    "        net[1].collect_params().initialize(init=mx.init.Xavier())\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T06:57:44.590359Z",
     "start_time": "2018-03-27T06:57:44.469220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import mxnet as mx\n",
    "\n",
    "def get_loss(data, net, ctx):\n",
    "    loss = 0.0\n",
    "    for feas, label in data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(feas.as_in_context(ctx))\n",
    "        cross_entropy = softmax_cross_entropy(output, label)\n",
    "        loss += nd.mean(cross_entropy).asscalar()\n",
    "    return loss / len(data)\n",
    "\n",
    "\n",
    "def accuracy(output, label):\n",
    "     return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "def evaluate_accuracy(net, data_iter, ctx=mx.cpu()):\n",
    "    step = len(data_iter)\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    for data, label in data_iter:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        output_loss = softmax_cross_entropy(output, label)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(output_loss).asscalar()\n",
    "    return loss / step, acc / step\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9,\n",
    "                                      'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss, valid_acc = evaluate_accuracy(net, valid_data, ctx)\n",
    "            epoch_str = (\"Epoch %d. Train loss: %.4f, Train Acc: %.4f, Valid loss %.4f, Valid Acc: %.4f, \"\n",
    "                         % (epoch, train_loss / len(train_data), 100 * train_acc / len(train_data),  \n",
    "                            valid_loss, 100 * valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, train_loss / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:51:11.874558Z",
     "start_time": "2018-03-27T06:57:45.417791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 3.9743, Train Acc: 16.5479, Valid loss 2.2622, Valid Acc: 61.1086, Time 00:01:45, lr 0.01\n",
      "Epoch 1. Train loss: 1.9775, Train Acc: 51.4922, Valid loss 1.0434, Valid Acc: 78.6480, Time 00:01:55, lr 0.01\n",
      "Epoch 2. Train loss: 1.2745, Train Acc: 65.0678, Valid loss 0.7323, Valid Acc: 83.8639, Time 00:01:51, lr 0.01\n",
      "Epoch 3. Train loss: 1.0462, Train Acc: 70.4196, Valid loss 0.6241, Valid Acc: 85.4974, Time 00:01:53, lr 0.01\n",
      "Epoch 4. Train loss: 0.9138, Train Acc: 74.0666, Valid loss 0.5655, Valid Acc: 85.8880, Time 00:01:53, lr 0.01\n",
      "Epoch 5. Train loss: 0.8365, Train Acc: 75.9121, Valid loss 0.5047, Valid Acc: 87.2374, Time 00:01:53, lr 0.01\n",
      "Epoch 6. Train loss: 0.7748, Train Acc: 76.9522, Valid loss 0.4657, Valid Acc: 88.0144, Time 00:01:51, lr 0.01\n",
      "Epoch 7. Train loss: 0.7403, Train Acc: 78.3340, Valid loss 0.4325, Valid Acc: 88.2587, Time 00:01:52, lr 0.01\n",
      "Epoch 8. Train loss: 0.6920, Train Acc: 79.2745, Valid loss 0.4045, Valid Acc: 89.0754, Time 00:01:54, lr 0.01\n",
      "Epoch 9. Train loss: 0.6580, Train Acc: 80.0902, Valid loss 0.3872, Valid Acc: 89.9320, Time 00:01:52, lr 0.01\n",
      "Epoch 10. Train loss: 0.6325, Train Acc: 80.7069, Valid loss 0.3756, Valid Acc: 89.5898, Time 00:01:54, lr 0.01\n",
      "Epoch 11. Train loss: 0.6101, Train Acc: 81.4020, Valid loss 0.3472, Valid Acc: 90.7800, Time 00:01:55, lr 0.01\n",
      "Epoch 12. Train loss: 0.5871, Train Acc: 82.3622, Valid loss 0.3372, Valid Acc: 91.1620, Time 00:01:50, lr 0.01\n",
      "Epoch 13. Train loss: 0.5734, Train Acc: 82.3230, Valid loss 0.3176, Valid Acc: 91.6408, Time 00:01:50, lr 0.01\n",
      "Epoch 14. Train loss: 0.5553, Train Acc: 83.1078, Valid loss 0.3138, Valid Acc: 91.7700, Time 00:01:54, lr 0.01\n",
      "Epoch 15. Train loss: 0.5345, Train Acc: 83.2473, Valid loss 0.3040, Valid Acc: 92.1337, Time 00:01:52, lr 0.01\n",
      "Epoch 16. Train loss: 0.5233, Train Acc: 83.8514, Valid loss 0.3131, Valid Acc: 91.6096, Time 00:01:52, lr 0.01\n",
      "Epoch 17. Train loss: 0.5069, Train Acc: 84.6181, Valid loss 0.2775, Valid Acc: 92.6890, Time 00:01:51, lr 0.01\n",
      "Epoch 18. Train loss: 0.4935, Train Acc: 84.8752, Valid loss 0.2804, Valid Acc: 92.6578, Time 00:01:52, lr 0.01\n",
      "Epoch 19. Train loss: 0.4879, Train Acc: 84.8931, Valid loss 0.2603, Valid Acc: 93.0796, Time 00:01:53, lr 0.01\n",
      "Epoch 20. Train loss: 0.4709, Train Acc: 85.5311, Valid loss 0.2557, Valid Acc: 93.2614, Time 00:01:53, lr 0.01\n",
      "Epoch 21. Train loss: 0.4622, Train Acc: 85.2836, Valid loss 0.2508, Valid Acc: 93.3723, Time 00:01:54, lr 0.01\n",
      "Epoch 22. Train loss: 0.4536, Train Acc: 86.0813, Valid loss 0.2313, Valid Acc: 93.8963, Time 00:01:51, lr 0.01\n",
      "Epoch 23. Train loss: 0.4392, Train Acc: 86.3368, Valid loss 0.2313, Valid Acc: 93.9319, Time 00:01:53, lr 0.01\n",
      "Epoch 24. Train loss: 0.4223, Train Acc: 86.7779, Valid loss 0.2301, Valid Acc: 94.2203, Time 00:01:53, lr 0.01\n",
      "Epoch 25. Train loss: 0.4107, Train Acc: 87.7136, Valid loss 0.2193, Valid Acc: 94.6109, Time 00:01:53, lr 0.01\n",
      "Epoch 26. Train loss: 0.4080, Train Acc: 87.3574, Valid loss 0.2112, Valid Acc: 94.5356, Time 00:01:51, lr 0.01\n",
      "Epoch 27. Train loss: 0.4061, Train Acc: 87.4711, Valid loss 0.2012, Valid Acc: 95.2415, Time 00:01:52, lr 0.01\n",
      "Epoch 28. Train loss: 0.3949, Train Acc: 87.3995, Valid loss 0.2071, Valid Acc: 94.6195, Time 00:01:52, lr 0.01\n",
      "Epoch 29. Train loss: 0.3902, Train Acc: 87.7283, Valid loss 0.2009, Valid Acc: 94.9391, Time 00:01:51, lr 0.01\n",
      "Epoch 30. Train loss: 0.3628, Train Acc: 88.8562, Valid loss 0.1838, Valid Acc: 95.7785, Time 00:01:51, lr 0.005\n",
      "Epoch 31. Train loss: 0.3535, Train Acc: 89.1852, Valid loss 0.1798, Valid Acc: 95.9872, Time 00:01:51, lr 0.005\n",
      "Epoch 32. Train loss: 0.3489, Train Acc: 89.4276, Valid loss 0.1744, Valid Acc: 96.2358, Time 00:01:52, lr 0.005\n",
      "Epoch 33. Train loss: 0.3375, Train Acc: 89.8670, Valid loss 0.1706, Valid Acc: 95.8097, Time 00:01:52, lr 0.005\n",
      "Epoch 34. Train loss: 0.3456, Train Acc: 89.3006, Valid loss 0.1743, Valid Acc: 95.9958, Time 00:01:52, lr 0.005\n",
      "Epoch 35. Train loss: 0.3319, Train Acc: 89.7871, Valid loss 0.1711, Valid Acc: 95.7515, Time 00:01:51, lr 0.005\n",
      "Epoch 36. Train loss: 0.3400, Train Acc: 89.4764, Valid loss 0.1688, Valid Acc: 96.0938, Time 00:01:52, lr 0.005\n",
      "Epoch 37. Train loss: 0.3246, Train Acc: 90.0458, Valid loss 0.1591, Valid Acc: 96.4489, Time 00:01:53, lr 0.005\n",
      "Epoch 38. Train loss: 0.3140, Train Acc: 90.3176, Valid loss 0.1609, Valid Acc: 96.0356, Time 00:01:51, lr 0.005\n",
      "Epoch 39. Train loss: 0.3234, Train Acc: 90.2965, Valid loss 0.1621, Valid Acc: 96.3864, Time 00:01:53, lr 0.005\n",
      "Epoch 40. Train loss: 0.3265, Train Acc: 90.1518, Valid loss 0.1567, Valid Acc: 96.3154, Time 00:01:51, lr 0.005\n",
      "Epoch 41. Train loss: 0.3204, Train Acc: 90.2754, Valid loss 0.1563, Valid Acc: 96.6974, Time 00:01:55, lr 0.005\n",
      "Epoch 42. Train loss: 0.3244, Train Acc: 90.1078, Valid loss 0.1575, Valid Acc: 96.5995, Time 00:01:54, lr 0.005\n",
      "Epoch 43. Train loss: 0.3203, Train Acc: 90.2900, Valid loss 0.1519, Valid Acc: 96.8126, Time 00:01:53, lr 0.005\n",
      "Epoch 44. Train loss: 0.3060, Train Acc: 90.6694, Valid loss 0.1476, Valid Acc: 96.9815, Time 00:01:54, lr 0.005\n",
      "Epoch 45. Train loss: 0.3084, Train Acc: 90.9769, Valid loss 0.1486, Valid Acc: 96.4973, Time 00:01:53, lr 0.005\n",
      "Epoch 46. Train loss: 0.3125, Train Acc: 90.7944, Valid loss 0.1454, Valid Acc: 96.8395, Time 00:01:53, lr 0.005\n",
      "Epoch 47. Train loss: 0.3049, Train Acc: 90.9933, Valid loss 0.1465, Valid Acc: 96.7061, Time 00:01:54, lr 0.005\n",
      "Epoch 48. Train loss: 0.3012, Train Acc: 91.1625, Valid loss 0.1471, Valid Acc: 96.9148, Time 00:01:54, lr 0.005\n",
      "Epoch 49. Train loss: 0.3017, Train Acc: 90.9915, Valid loss 0.1395, Valid Acc: 96.7373, Time 00:01:53, lr 0.005\n",
      "Epoch 50. Train loss: 0.2965, Train Acc: 91.0046, Valid loss 0.1419, Valid Acc: 96.7017, Time 00:01:56, lr 0.005\n",
      "Epoch 51. Train loss: 0.2986, Train Acc: 91.2763, Valid loss 0.1377, Valid Acc: 97.0213, Time 00:01:53, lr 0.005\n",
      "Epoch 52. Train loss: 0.2967, Train Acc: 91.3382, Valid loss 0.1375, Valid Acc: 97.4432, Time 00:01:55, lr 0.005\n",
      "Epoch 53. Train loss: 0.2936, Train Acc: 91.1477, Valid loss 0.1342, Valid Acc: 97.4077, Time 00:01:53, lr 0.005\n",
      "Epoch 54. Train loss: 0.2900, Train Acc: 90.9638, Valid loss 0.1353, Valid Acc: 97.0213, Time 00:01:54, lr 0.005\n",
      "Epoch 55. Train loss: 0.2854, Train Acc: 91.3496, Valid loss 0.1346, Valid Acc: 97.1634, Time 00:01:53, lr 0.005\n",
      "Epoch 56. Train loss: 0.2825, Train Acc: 91.6117, Valid loss 0.1314, Valid Acc: 97.4787, Time 00:01:52, lr 0.005\n",
      "Epoch 57. Train loss: 0.2816, Train Acc: 91.4604, Valid loss 0.1325, Valid Acc: 97.4432, Time 00:01:52, lr 0.005\n",
      "Epoch 58. Train loss: 0.2824, Train Acc: 91.5481, Valid loss 0.1309, Valid Acc: 97.2699, Time 00:01:54, lr 0.005\n",
      "Epoch 59. Train loss: 0.2793, Train Acc: 91.8461, Valid loss 0.1319, Valid Acc: 97.1989, Time 00:01:53, lr 0.005\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "ctx = mx.gpu()\n",
    "num_epochs = 60\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lr_period = 30\n",
    "lr_decay = 0.5\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate,\n",
    "      weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
