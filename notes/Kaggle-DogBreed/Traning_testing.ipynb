{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T01:52:45.151616Z",
     "start_time": "2018-03-27T01:52:44.308630Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T01:53:29.931653Z",
     "start_time": "2018-03-27T01:53:29.625858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Size:  (10222, 2048)\n",
      "X_test Size:  (10357, 2048)\n",
      "y_train Size:  (10222,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/home/yansheng/kaggle_dog/resnet50_v2_pretrained_Xy.h5', 'r') as f:\n",
    "    X_train_resnet50_v2 = np.array(f['X_train_resnet50_v2'])\n",
    "    X_test_resnet50_v2 = np.array(f['X_test_resnet50_v2'])\n",
    "    y_train = np.array(f['y_train'])\n",
    "    \n",
    "print (\"X_train Size: \", X_train_resnet50_v2.shape)\n",
    "print (\"X_test Size: \", X_test_resnet50_v2.shape)\n",
    "print (\"y_train Size: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T01:54:07.541313Z",
     "start_time": "2018-03-27T01:54:07.395243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resnet50_v2, y_train, test_size=0.2)\n",
    "\n",
    "# dataset\n",
    "dataset_train = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = mx.gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "# data itet\n",
    "batch_size = 128\n",
    "data_iter_train = mx.gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = mx.gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T01:54:22.478604Z",
     "start_time": "2018-03-27T01:54:22.459269Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "def get_net(ctx):\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation='relu'))\n",
    "        net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T02:11:32.505524Z",
     "start_time": "2018-03-27T02:11:32.468578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def accuracy(output, label):\n",
    "     return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "def evaluate_accuracy(net, data_iter, ctx=mx.cpu()):\n",
    "    step = len(data_iter)\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    for data, label in data_iter:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        output_loss = softmax_cross_entropy(output, label)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(output_loss).asscalar()\n",
    "    return loss / step, acc / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T02:11:33.061284Z",
     "start_time": "2018-03-27T02:11:32.966076Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data_iter_train, data_iter_val, ctx, \n",
    "          epochs=50, lr=0.01, mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20):\n",
    "\n",
    "    \n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),  'sgd', {'learning_rate': lr, 'momentum': mome, \n",
    "                                      'wd': wd})\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        steps = len(data_iter_train)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in data_iter_train:\n",
    "\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                out = net(X)\n",
    "                loss = softmax_cross_entropy(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(out, y)\n",
    "\n",
    "        val_loss, val_acc = evaluate_accuracy(net, data_iter_val, ctx)\n",
    "        train_loss_list.append(train_loss / steps)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T02:12:12.176157Z",
     "start_time": "2018-03-27T02:11:33.656220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.1511, acc: 13.91%, val_loss 2.7966, val_acc 53.20%\n",
      "Epoch 2. loss: 2.0994, acc: 50.08%, val_loss 1.1548, val_acc 77.45%\n",
      "Epoch 3. loss: 1.2252, acc: 66.66%, val_loss 0.7859, val_acc 80.58%\n",
      "Epoch 4. loss: 0.9540, acc: 72.90%, val_loss 0.6525, val_acc 83.42%\n",
      "Epoch 5. loss: 0.8313, acc: 75.47%, val_loss 0.6044, val_acc 82.24%\n",
      "Epoch 6. loss: 0.7371, acc: 78.05%, val_loss 0.5689, val_acc 83.42%\n",
      "Epoch 7. loss: 0.6997, acc: 78.93%, val_loss 0.5356, val_acc 83.08%\n",
      "Epoch 8. loss: 0.6491, acc: 79.94%, val_loss 0.5264, val_acc 83.42%\n",
      "Epoch 9. loss: 0.5949, acc: 82.02%, val_loss 0.5186, val_acc 83.51%\n",
      "Epoch 10. loss: 0.5752, acc: 82.09%, val_loss 0.5088, val_acc 84.05%\n",
      "Epoch 11. loss: 0.5577, acc: 82.77%, val_loss 0.4907, val_acc 84.49%\n",
      "Epoch 12. loss: 0.5239, acc: 83.79%, val_loss 0.4845, val_acc 84.15%\n",
      "Epoch 13. loss: 0.5107, acc: 84.28%, val_loss 0.4849, val_acc 83.90%\n",
      "Epoch 14. loss: 0.4938, acc: 84.86%, val_loss 0.4709, val_acc 84.84%\n",
      "Epoch 15. loss: 0.4808, acc: 84.63%, val_loss 0.4773, val_acc 84.88%\n",
      "Epoch 16. loss: 0.4623, acc: 85.47%, val_loss 0.4855, val_acc 84.24%\n",
      "Epoch 17. loss: 0.4544, acc: 85.57%, val_loss 0.4776, val_acc 84.83%\n",
      "Epoch 18. loss: 0.4346, acc: 86.24%, val_loss 0.4660, val_acc 85.08%\n",
      "Epoch 19. loss: 0.4316, acc: 86.23%, val_loss 0.4667, val_acc 84.59%\n",
      "Epoch 20. loss: 0.4116, acc: 86.59%, val_loss 0.4656, val_acc 84.79%\n",
      "Epoch 21. loss: 0.3969, acc: 87.51%, val_loss 0.4576, val_acc 85.42%\n",
      "Epoch 22. loss: 0.3698, acc: 88.20%, val_loss 0.4579, val_acc 84.79%\n",
      "Epoch 23. loss: 0.3714, acc: 88.37%, val_loss 0.4553, val_acc 84.79%\n",
      "Epoch 24. loss: 0.3698, acc: 88.17%, val_loss 0.4529, val_acc 85.08%\n",
      "Epoch 25. loss: 0.3656, acc: 88.66%, val_loss 0.4554, val_acc 84.39%\n",
      "Epoch 26. loss: 0.3630, acc: 88.32%, val_loss 0.4501, val_acc 85.71%\n",
      "Epoch 27. loss: 0.3424, acc: 89.57%, val_loss 0.4544, val_acc 85.13%\n",
      "Epoch 28. loss: 0.3547, acc: 89.01%, val_loss 0.4478, val_acc 85.23%\n",
      "Epoch 29. loss: 0.3431, acc: 89.51%, val_loss 0.4502, val_acc 85.27%\n",
      "Epoch 30. loss: 0.3475, acc: 89.08%, val_loss 0.4507, val_acc 85.18%\n",
      "Epoch 31. loss: 0.3429, acc: 89.06%, val_loss 0.4540, val_acc 84.59%\n",
      "Epoch 32. loss: 0.3328, acc: 89.64%, val_loss 0.4531, val_acc 84.89%\n",
      "Epoch 33. loss: 0.3361, acc: 89.43%, val_loss 0.4476, val_acc 85.28%\n",
      "Epoch 34. loss: 0.3260, acc: 89.55%, val_loss 0.4496, val_acc 84.79%\n",
      "Epoch 35. loss: 0.3207, acc: 89.93%, val_loss 0.4507, val_acc 85.03%\n",
      "Epoch 36. loss: 0.3263, acc: 89.70%, val_loss 0.4513, val_acc 85.62%\n",
      "Epoch 37. loss: 0.3204, acc: 89.81%, val_loss 0.4533, val_acc 84.59%\n",
      "Epoch 38. loss: 0.3059, acc: 90.23%, val_loss 0.4505, val_acc 84.79%\n",
      "Epoch 39. loss: 0.2993, acc: 90.65%, val_loss 0.4534, val_acc 84.69%\n",
      "Epoch 40. loss: 0.3012, acc: 90.48%, val_loss 0.4511, val_acc 85.62%\n",
      "Epoch 41. loss: 0.2910, acc: 90.92%, val_loss 0.4472, val_acc 85.23%\n",
      "Epoch 42. loss: 0.2870, acc: 90.99%, val_loss 0.4444, val_acc 85.22%\n",
      "Epoch 43. loss: 0.2842, acc: 91.03%, val_loss 0.4480, val_acc 85.13%\n",
      "Epoch 44. loss: 0.2940, acc: 90.71%, val_loss 0.4458, val_acc 84.98%\n",
      "Epoch 45. loss: 0.2888, acc: 91.24%, val_loss 0.4482, val_acc 85.13%\n",
      "Epoch 46. loss: 0.2895, acc: 91.05%, val_loss 0.4486, val_acc 85.03%\n",
      "Epoch 47. loss: 0.2922, acc: 90.90%, val_loss 0.4469, val_acc 85.13%\n",
      "Epoch 48. loss: 0.2819, acc: 91.31%, val_loss 0.4468, val_acc 85.03%\n",
      "Epoch 49. loss: 0.2762, acc: 91.39%, val_loss 0.4450, val_acc 85.08%\n",
      "Epoch 50. loss: 0.2774, acc: 91.51%, val_loss 0.4445, val_acc 84.98%\n",
      "Epoch 51. loss: 0.2779, acc: 91.30%, val_loss 0.4454, val_acc 85.47%\n",
      "Epoch 52. loss: 0.2745, acc: 91.55%, val_loss 0.4476, val_acc 85.08%\n",
      "Epoch 53. loss: 0.2797, acc: 91.08%, val_loss 0.4467, val_acc 85.13%\n",
      "Epoch 54. loss: 0.2791, acc: 91.55%, val_loss 0.4469, val_acc 84.98%\n",
      "Epoch 55. loss: 0.2771, acc: 91.40%, val_loss 0.4473, val_acc 85.13%\n",
      "Epoch 56. loss: 0.2682, acc: 91.61%, val_loss 0.4454, val_acc 85.03%\n",
      "Epoch 57. loss: 0.2708, acc: 91.46%, val_loss 0.4484, val_acc 85.23%\n",
      "Epoch 58. loss: 0.2750, acc: 91.50%, val_loss 0.4474, val_acc 84.74%\n",
      "Epoch 59. loss: 0.2672, acc: 91.30%, val_loss 0.4475, val_acc 85.13%\n",
      "Epoch 60. loss: 0.2630, acc: 92.08%, val_loss 0.4471, val_acc 85.03%\n",
      "Epoch 61. loss: 0.2653, acc: 91.51%, val_loss 0.4466, val_acc 85.18%\n",
      "Epoch 62. loss: 0.2680, acc: 91.72%, val_loss 0.4462, val_acc 85.28%\n",
      "Epoch 63. loss: 0.2606, acc: 92.05%, val_loss 0.4465, val_acc 85.13%\n",
      "Epoch 64. loss: 0.2586, acc: 92.23%, val_loss 0.4461, val_acc 84.98%\n",
      "Epoch 65. loss: 0.2600, acc: 92.04%, val_loss 0.4456, val_acc 84.98%\n",
      "Epoch 66. loss: 0.2579, acc: 92.02%, val_loss 0.4460, val_acc 85.23%\n",
      "Epoch 67. loss: 0.2562, acc: 92.01%, val_loss 0.4450, val_acc 85.13%\n",
      "Epoch 68. loss: 0.2536, acc: 91.99%, val_loss 0.4464, val_acc 85.28%\n",
      "Epoch 69. loss: 0.2595, acc: 91.98%, val_loss 0.4468, val_acc 84.98%\n",
      "Epoch 70. loss: 0.2613, acc: 91.83%, val_loss 0.4459, val_acc 85.03%\n",
      "Epoch 71. loss: 0.2564, acc: 92.15%, val_loss 0.4472, val_acc 85.18%\n",
      "Epoch 72. loss: 0.2566, acc: 91.94%, val_loss 0.4460, val_acc 85.32%\n",
      "Epoch 73. loss: 0.2536, acc: 92.06%, val_loss 0.4452, val_acc 85.37%\n",
      "Epoch 74. loss: 0.2514, acc: 92.13%, val_loss 0.4472, val_acc 84.93%\n",
      "Epoch 75. loss: 0.2503, acc: 92.53%, val_loss 0.4483, val_acc 84.69%\n",
      "Epoch 76. loss: 0.2556, acc: 92.13%, val_loss 0.4491, val_acc 85.18%\n",
      "Epoch 77. loss: 0.2507, acc: 92.46%, val_loss 0.4474, val_acc 85.28%\n",
      "Epoch 78. loss: 0.2521, acc: 92.08%, val_loss 0.4490, val_acc 84.98%\n",
      "Epoch 79. loss: 0.2501, acc: 92.12%, val_loss 0.4478, val_acc 84.98%\n",
      "Epoch 80. loss: 0.2512, acc: 92.48%, val_loss 0.4471, val_acc 84.79%\n",
      "Epoch 81. loss: 0.2503, acc: 92.16%, val_loss 0.4467, val_acc 85.13%\n",
      "Epoch 82. loss: 0.2500, acc: 92.57%, val_loss 0.4468, val_acc 85.13%\n",
      "Epoch 83. loss: 0.2482, acc: 92.60%, val_loss 0.4464, val_acc 85.18%\n",
      "Epoch 84. loss: 0.2519, acc: 92.44%, val_loss 0.4475, val_acc 85.23%\n",
      "Epoch 85. loss: 0.2467, acc: 92.45%, val_loss 0.4473, val_acc 85.23%\n",
      "Epoch 86. loss: 0.2460, acc: 92.54%, val_loss 0.4481, val_acc 85.23%\n",
      "Epoch 87. loss: 0.2472, acc: 92.29%, val_loss 0.4479, val_acc 85.18%\n",
      "Epoch 88. loss: 0.2401, acc: 92.26%, val_loss 0.4485, val_acc 85.23%\n",
      "Epoch 89. loss: 0.2517, acc: 92.03%, val_loss 0.4480, val_acc 85.03%\n",
      "Epoch 90. loss: 0.2462, acc: 92.33%, val_loss 0.4471, val_acc 84.98%\n",
      "Epoch 91. loss: 0.2501, acc: 91.99%, val_loss 0.4478, val_acc 85.03%\n",
      "Epoch 92. loss: 0.2452, acc: 92.48%, val_loss 0.4473, val_acc 85.23%\n",
      "Epoch 93. loss: 0.2467, acc: 92.53%, val_loss 0.4471, val_acc 85.33%\n",
      "Epoch 94. loss: 0.2401, acc: 93.00%, val_loss 0.4468, val_acc 85.33%\n",
      "Epoch 95. loss: 0.2449, acc: 92.45%, val_loss 0.4465, val_acc 85.18%\n",
      "Epoch 96. loss: 0.2463, acc: 92.47%, val_loss 0.4464, val_acc 85.08%\n",
      "Epoch 97. loss: 0.2381, acc: 92.76%, val_loss 0.4463, val_acc 85.23%\n",
      "Epoch 98. loss: 0.2392, acc: 92.51%, val_loss 0.4460, val_acc 85.23%\n",
      "Epoch 99. loss: 0.2459, acc: 92.14%, val_loss 0.4466, val_acc 85.23%\n",
      "Epoch 100. loss: 0.2421, acc: 92.58%, val_loss 0.4466, val_acc 85.08%\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "train_loss_list, val_loss_list = train(net, data_iter_train, data_iter_val, ctx, epochs=100, lr=0.01, \\\n",
    "      mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
