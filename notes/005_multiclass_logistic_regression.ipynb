{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概览\n",
    "\n",
    "与前面的线性回归相比，你会发现多类逻辑回归教程的结构跟其非常相似：**获取数据**、**定义模型**及**优化算法和求解**。\n",
    "\n",
    "事实上，几乎所有的实际神经网络应用都有着同样结构。他们的主要区别在于模型的类型和数据的规模。每一两年会有一个新的优化算法出来，但它们基本都是随机梯度下降的变种。\n",
    "\n",
    "通过本课学习到的技能：\n",
    "\n",
    "- 多分类问题的损失函数与准确度度量函数，它们与回归问题中的差别定义\n",
    "- Softmax的数值稳定问题\n",
    "- 为什么trainer总是传入一个batchsize作为参数\n",
    "- Gluon中使用GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:51.114145Z",
     "start_time": "2017-12-04T05:04:49.891074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype('float32')/255, label.astype('float32')\n",
    "\n",
    "mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:51.128484Z",
     "start_time": "2017-12-04T05:04:51.115974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('example shape: ', (28, 28, 1), 'label:', 2.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = mnist_train[0]\n",
    "('example shape: ', data.shape, 'label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:51.233475Z",
     "start_time": "2017-12-04T05:04:51.130648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:52.630429Z",
     "start_time": "2017-12-04T05:04:51.236478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 28, 28, 1) (256,)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    print(data.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:52.640370Z",
     "start_time": "2017-12-04T05:04:52.632092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=(num_outputs,))\n",
    "\n",
    "params=[W, b]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:52.749495Z",
     "start_time": "2017-12-04T05:04:52.646557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(score):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    - score N*D的矩阵，N代表样本的个数，D代表特征的长度\n",
    "    output:\n",
    "    - prob N*D的矩阵，每个元素代表了概率\n",
    "    \"\"\"\n",
    "    max_value = nd.max(score, axis=1, keepdims=True)\n",
    "    score = score - max_value\n",
    "    exp_score = nd.exp(score)\n",
    "    prob = exp_score / nd.sum(exp_score, axis=1, keepdims=True)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:52.871962Z",
     "start_time": "2017-12-04T05:04:52.752140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义model\n",
    "def mlr(X):\n",
    "    return softmax(nd.dot(X.reshape((X.shape[0], -1)), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:53.005625Z",
     "start_time": "2017-12-04T05:04:52.874760Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义loss\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    return - nd.pick(nd.log(y), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:04:53.139718Z",
     "start_time": "2017-12-04T05:04:53.008739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义准确率计算函数\n",
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0.\n",
    "    for data, label in data_iterator:\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "    return acc / len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:05:39.066365Z",
     "start_time": "2017-12-04T05:04:53.143167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10048828125000001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data, mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:05:39.072646Z",
     "start_time": "2017-12-04T05:05:39.068636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义优化算法\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:33:33.247068Z",
     "start_time": "2017-12-04T05:05:39.074245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.700600, Train acc 0.438481, Test acc 0.569531\n",
      "Epoch 1. Loss: 1.896220, Train acc 0.618440, Test acc 0.647754\n",
      "Epoch 2. Loss: 1.573088, Train acc 0.667520, Test acc 0.681934\n",
      "Epoch 3. Loss: 1.401253, Train acc 0.695728, Test acc 0.704492\n",
      "Epoch 4. Loss: 1.289541, Train acc 0.713863, Test acc 0.716699\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "learning_rate = .1\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = mlr(data)\n",
    "            loss = cross_entropy_loss(output, label)\n",
    "        loss.backward()\n",
    "        # 将梯度做平均，这样学习率会对batch size不那么敏感\n",
    "        SGD(params, learning_rate / batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, mlr)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考：为什么测试集的准确率比训练集的准确率高呢？\n",
    "\n",
    "> 统计学习理论：基于独立同分布假设，训练误差的期望值不高于测试集误差的期望值。注意，这是期望值（单次实验结果可以是随机的）。当然，有很多实践方法（如正则化）可以使测试集准确率更大。\n",
    "\n",
    "> 由于模型相对简单，并没有Overfiting的现象，所以模型有较大的bias error，这时测试集可能会有低的error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T04:38:07.528799Z",
     "start_time": "2017-12-04T04:38:07.522733Z"
    }
   },
   "source": [
    "尝试增大学习率，你会发现结果马上回变成很糟糕，精度基本徘徊在随机的0.1左右。这是为什么呢？提示：\n",
    "\n",
    "- 打印下output看看是不是有有什么异常\n",
    "- 前面线性回归还好好的，这里我们在net()里加了什么呢？\n",
    "- 如果给exp输入个很大的数会怎么样？\n",
    "- 即使解决exp的问题，求出来的导数是不是还是不稳定？\n",
    "\n",
    "请仔细想想再去对比下我们小伙伴之一@pluskid早年写的一篇[blog](http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/)解释这个问题，看看你想的是不是不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluon 的版本， 使用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:33:33.255563Z",
     "start_time": "2017-12-04T05:33:33.249297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0.\n",
    "    for data, label in data_iterator:\n",
    "        output = net(data.as_in_context(mx.gpu()))\n",
    "        acc += accuracy(output, label.as_in_context(mx.gpu()))\n",
    "    return acc / len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:35:13.863567Z",
     "start_time": "2017-12-04T05:33:33.257228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.798806, Train acc 0.740481, Test acc 0.803418\n",
      "Epoch 1. Loss: 0.576095, Train acc 0.809181, Test acc 0.817578\n",
      "Epoch 2. Loss: 0.530283, Train acc 0.823582, Test acc 0.827148\n",
      "Epoch 3. Loss: 0.506481, Train acc 0.829832, Test acc 0.835645\n",
      "Epoch 4. Loss: 0.491218, Train acc 0.833876, Test acc 0.838672\n",
      "Epoch 5. Loss: 0.477839, Train acc 0.838060, Test acc 0.837695\n",
      "Epoch 6. Loss: 0.469924, Train acc 0.840409, Test acc 0.841895\n",
      "Epoch 7. Loss: 0.462449, Train acc 0.843218, Test acc 0.845605\n",
      "Epoch 8. Loss: 0.456732, Train acc 0.844963, Test acc 0.849316\n",
      "Epoch 9. Loss: 0.451933, Train acc 0.845246, Test acc 0.848828\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "import mxnet as mx\n",
    "\n",
    "# 定义模型\n",
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Flatten())\n",
    "net.add(gluon.nn.Dense(10))\n",
    "net.initialize(ctx=mx.gpu())\n",
    "\n",
    "# 定义loss\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# 优化器\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "\n",
    "# 训练\n",
    "for epoch in range(10):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(mx.gpu()))\n",
    "            loss = softmax_cross_entropy(output, label.as_in_context(mx.gpu()))\n",
    "        loss.backward()\n",
    "        # trainer里总是传入batch size的原因是什么呢，为什么不直接在初始化trainer时写入呢。\n",
    "        # 将梯度做平均，这样学习率会对batch size不那么敏感\n",
    "        trainer.step(batch_size) \n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label.as_in_context(mx.gpu()))\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讨论Gluon版本的准确率更高的原因\n",
    "\n",
    "1. net.initialize()：自带的初始化可能跟我们的随机初始化不完全一样，可能是更合适的。\n",
    "2. softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss() 将softmax与交叉熵放在一起，数值更加稳定，也就降低了之前对softmax函数直接求导可能带来的风险。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
