{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet的新接口Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么要开发Gluon的接口\n",
    "\n",
    "在MXNet中我们可以通过`Sybmol`模块来定义神经网络，并组通过`Module`模块提供的一些上层API来简化整个训练过程。那MXNet为什么还要重新开发一套Python的API呢，是否是重复造轮子呢？答案是否定的，Gluon主要是学习了Keras、Pytorch等框架的优点，支持动态图（Imperative）编程，更加灵活且方便调试。而原来MXNet基于Symbol来构建网络的方法是像TF、Caffe2一样静态图的编程方法。同时Gluon也继续了MXNet在静态图上的一些优化，比如节省显存，并行效率高等，运行起来比Pytorch更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更加简洁的接口\n",
    "\n",
    "我们先来看一下用Gluon的接口，如果创建并组训练一个神经网络的，我们以mnist数据集为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:17:25.694068Z",
     "start_time": "2018-03-16T07:17:25.686788Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import gluon\n",
    "import mxnet.gluon.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的读取\n",
    "\n",
    "首先我们利用Gluon的data模块来读取mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T04:06:24.672162Z",
     "start_time": "2018-03-16T04:06:10.625741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/yansheng/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading /home/yansheng/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype('float32') / 255, label.astype('float32')\n",
    "\n",
    "minist_train_dataset = gluon.data.vision.MNIST(train=True, transform=transform)\n",
    "minist_test_dataset = gluon.data.vision.MNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:17:42.408798Z",
     "start_time": "2018-03-16T07:17:42.400986Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(dataset=minist_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_data = gluon.data.DataLoader(dataset=minist_train_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:31:15.576449Z",
     "start_time": "2018-03-16T07:31:15.569834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(train_data)\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T06:42:44.302339Z",
     "start_time": "2018-03-16T06:42:44.276818Z"
    }
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "这里我们使用Gluon来定义一个LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:49:01.911121Z",
     "start_time": "2018-03-16T07:49:01.861203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step1 定义模型\n",
    "lenet = nn.Sequential()\n",
    "with lenet.name_scope():\n",
    "    lenet.add(nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    lenet.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    lenet.add(nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    lenet.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    lenet.add(nn.Flatten())\n",
    "    lenet.add(nn.Dense(128, activation='relu'))\n",
    "    lenet.add(nn.Dense(10))\n",
    "# Step2 初始化模型参数\n",
    "lenet.initialize(ctx=mx.gpu())\n",
    "# Step3 定义loss\n",
    "softmax_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# Step4 优化\n",
    "trainer = gluon.Trainer(lenet.collect_params(), 'sgd', {'learning_rate': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:49:02.546855Z",
     "start_time": "2018-03-16T07:49:02.522245Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "     return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    acc = 0\n",
    "    for data, label in data_iter:\n",
    "        data = data.transpose((0,3,1,2))\n",
    "        data = data.as_in_context(mx.gpu())\n",
    "        label = label.as_in_context(mx.gpu())\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "    return acc / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T07:50:52.510165Z",
     "start_time": "2018-03-16T07:49:03.342381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, test accuracy: 0.981110, average loss: 0.337135\n",
      "Epoch 1, test accuracy: 0.982859, average loss: 0.060181\n",
      "Epoch 2, test accuracy: 0.991571, average loss: 0.043981\n",
      "Epoch 3, test accuracy: 0.991205, average loss: 0.034054\n",
      "Epoch 4, test accuracy: 0.994320, average loss: 0.026045\n"
     ]
    }
   ],
   "source": [
    "import mxnet.autograd as ag\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data, label in train_data:\n",
    "        data = data.transpose((0,3,1,2))\n",
    "        data = data.as_in_context(mx.gpu())\n",
    "        label = label.as_in_context(mx.gpu())\n",
    "        with ag.record():\n",
    "            output = lenet(data)\n",
    "            loss = softmax_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        total_loss += nd.mean(loss).asscalar()\n",
    "    print(\"Epoch %d, test accuracy: %f, average loss: %f\" % (e, evaluate_accuracy(lenet, test_data), total_loss/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背后的英雄 nn.Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们前面使用了`nn.Sequential`来定义一个模型，但是没有仔细介绍它，它其实是`nn.Block`的一个简单的形式。而`nn.Block`是一个一般化的部件。整个神经网络可以是一个`nn.Block`，单个层也是一个`nn.Block`。我们可以（近似）无限地嵌套`nn.Block`来构建新的`nn.Block`。`nn.Block`主要提供3个方向的功能：\n",
    "1. 存储参数\n",
    "2. 描述`forward`如何执行\n",
    "3. 自动求导 \n",
    "\n",
    "所以`nn.Sequential`是一个`nn.Block`的容器，它通过`add`来添加`nn.Block`。它自动生成`forward()`函数。一个简单实现看起来如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:02:47.844477Z",
     "start_time": "2018-03-16T08:02:47.824406Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sequential, self).__init__(**kwargs)\n",
    "    def add(self, block):\n",
    "        self._children.append(block)\n",
    "    def forward(self, x):\n",
    "        for block in self._children:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知道了`nn.Block`里的魔法后，我们就可以自定我们自己的`nn.Block`了，来实现不同的深度学习应用可能遇到的一些新的层。\n",
    "\n",
    "在`nn.Block`中参数都是以一种`Parameter`的对象，通过这个对象的`data()`和`grad()`来访问对应的数据和梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:08:41.557575Z",
     "start_time": "2018-03-16T08:08:41.542487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-0.06245833  0.03828365  0.03117204]\n",
       "  [-0.01312185 -0.00229232  0.06483314]\n",
       "  [ 0.04128969 -0.03030807  0.03024808]]\n",
       " <NDArray 3x3 @cpu(0)>, \n",
       " [[ 0.  0.  0.]\n",
       "  [ 0.  0.  0.]\n",
       "  [ 0.  0.  0.]]\n",
       " <NDArray 3x3 @cpu(0)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_param = gluon.Parameter('my_params', shape=(3,3))\n",
    "my_param.initialize()\n",
    "(my_param.data(), my_param.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个`nn.Block`里都有一个类型为`ParameterDict`类型的成员变量`params`来保存所有这个层的参数。它其际上是一个名称到参数映射的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:11:41.906930Z",
     "start_time": "2018-03-16T08:11:41.895918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_layer_name (\n",
       "  Parameter custom_layer_namecustom_layer_param1 (shape=(3, 3), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = gluon.ParameterDict(prefix='custom_layer_name')\n",
    "pd.get('custom_layer_param1', shape=(3,3))\n",
    "pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自义我们自己的全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们要实现的功能在Gluon.nn模块中找不到对应的实现时，我们可以创建自己的层，它实际也就是一个`nn.Block`对象。要自定义一个`nn.Block`以，只需要继承`nn.Block`，如果该层需要参数，则在初始化函数中做好对应参数的初始化（实际只是分配的形状），然后再实现一个`forward()`函数来描述计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:13:24.721010Z",
     "start_time": "2018-03-16T08:13:24.697000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.weight = self.params.get(\n",
    "                'weight', shape=(in_units, units))\n",
    "            self.bias = self.params.get('bias', shape=(units,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = nd.dot(x, self.weight.data()) + self.bias.data()\n",
    "        return nd.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 审视模型的参数\n",
    "\n",
    "我们将从下面三个方面来详细讲解如何操作gluon定义的模型的参数。\n",
    "1. 初始化\n",
    "2. 读取参数\n",
    "3. 参数的保存与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:37:11.524434Z",
     "start_time": "2018-03-16T08:37:11.517367Z"
    }
   },
   "source": [
    "从上面我们们在mnist训练一个模型的步骤中可以看出，当我们定义好模型后，第一步就是需要调用`initialize()`对模型进行参数初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:14:08.693703Z",
     "start_time": "2018-03-16T09:14:08.675524Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(4, activation='relu'))\n",
    "        net.add(nn.Dense(2))\n",
    "    return net\n",
    "net = get_net()\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一直使用默认的`initialize`来初始化权重。实际上我们可以指定其他初始化的方法，`mxnet.initializer`模块中提供了大量的初始化权重的方法。比如非常流行的`Xavier`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:08:05.303963Z",
     "start_time": "2018-03-16T09:08:05.290075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  1.01732085e-05  -1.15762616e-03]\n",
       " [ -6.45724300e-04  -8.69403069e-04]\n",
       " [  9.93979815e-03  -4.62137396e-03]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net.initialize(init=mx.init.Xavier())\n",
    "x = nd.random.normal(shape=(3,4))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:39:46.643592Z",
     "start_time": "2018-03-16T08:39:46.637017Z"
    }
   },
   "source": [
    "我们可以`weight`和`bias`来访问Dense的参数，它们是`Parameter`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:09:56.951150Z",
     "start_time": "2018-03-16T09:09:56.928441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[ 0.05273546  0.03540857 -0.0246018   0.01501857]\n",
      " [ 0.05944719 -0.05561438  0.06647021 -0.0113181 ]\n",
      " [-0.00701939  0.06939382 -0.03820197  0.03745773]\n",
      " [-0.02916674  0.03676125  0.03868672  0.05910886]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "weight gradient \n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "bias gradient \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight\n",
    "b = net[0].bias\n",
    "print('weight:', w.data())\n",
    "print('weight gradient', w.grad())\n",
    "print('bias:', b.data())\n",
    "print('bias gradient', b.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T08:41:45.668565Z",
     "start_time": "2018-03-16T08:41:45.661680Z"
    }
   },
   "source": [
    "我们也可以通过`collect_params`来访问`Block`里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应`Parameter`的dict。既可以用正常`[]`来访问参数，也可以用`get()`，它不需要填写名字的前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:12:08.061467Z",
     "start_time": "2018-03-16T09:12:08.045669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential18_ (\n",
      "  Parameter sequential18_dense0_weight (shape=(4, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential18_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential18_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential18_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[[ 0.05273546  0.03540857 -0.0246018   0.01501857]\n",
      " [ 0.05944719 -0.05561438  0.06647021 -0.0113181 ]\n",
      " [-0.00701939  0.06939382 -0.03820197  0.03745773]\n",
      " [-0.02916674  0.03676125  0.03868672  0.05910886]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['sequential18_dense0_weight'].data())\n",
    "print(params.get('dense0_bias').data()) #不需要名字的前缀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延后的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:15:11.600595Z",
     "start_time": "2018-03-16T09:15:11.593906Z"
    }
   },
   "source": [
    "如果我们仔细分析过整个网络的初始化，我们会有发现，当我们没有给网络真正的输入数据时，网络中的很多参数是无法确认形状的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:17:12.099855Z",
     "start_time": "2018-03-16T09:17:12.091039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential20_ (\n",
       "  Parameter sequential20_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_net()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:17:24.309999Z",
     "start_time": "2018-03-16T09:17:24.299063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential20_ (\n",
       "  Parameter sequential20_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们注意到参数中的`weight`的形状的第二维都是0, 也就是说还没有确认。那我们可以肯定的是这些参数肯定是还没有分配内存的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:19:01.006226Z",
     "start_time": "2018-03-16T09:19:00.989380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential20_ (\n",
       "  Parameter sequential20_dense0_weight (shape=(4, 4), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential20_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们给这个网络一个输入数据后，网络中的数据参数的形状就固定下来了。而这个时候，如果我们给这个网络一个不同shape的输入数据，那运行中就会出现崩溃的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数的保存与加载 \n",
    "\n",
    "`gluon.Sequential`模块提供了`save`和`load`接口来方便我们对一个网络的参数进行保存与加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:23:59.815901Z",
     "start_time": "2018-03-16T09:23:59.791593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"mynet.params\"\n",
    "net.save_params(filename)\n",
    "net2 = get_net()\n",
    "net2.load_params(filename, mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybridize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面我们使用gluon来训练mnist，可以看出，我们使用的是一种命令式的编程风格。大部分的深度学习框架只在命令式与符号式间二选一。那我们能不能拿到两种泛式全部的优点呢，事实上这一点可以做到。在MXNet的GluonAPI中，我们可以使用`HybridBlock`或者`HybridSequential`来构建网络。默认他们跟`Block`和`Sequential`一样是命令式的。但当我们调用`.hybridize()`后，系统会转撚成符号式来执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:29:35.046217Z",
     "start_time": "2018-03-16T09:29:35.005947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.06729897 -0.15641479]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_net():\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.Dense(256, activation=\"relu\"),\n",
    "            nn.Dense(128, activation=\"relu\"),\n",
    "            nn.Dense(2)\n",
    "        )\n",
    "    net.initialize()\n",
    "    return net\n",
    "\n",
    "x = nd.random.normal(shape=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:29:48.948520Z",
     "start_time": "2018-03-16T09:29:48.917706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.06729897 -0.15641479]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hybridize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到只有继承自HybridBlock的层才会被优化。HybridSequential和Gluon提供的层都是它的子类。如果一个层只是继承自Block，那么我们将跳过优化。我们可以将符号化的模型的定义保存下来，在其他语言API中加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T09:33:57.819673Z",
     "start_time": "2018-03-16T09:33:57.810671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"data\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense0_weight\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(256, 0)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense0_bias\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__init__\": \"zeros\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(256,)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"FullyConnected\", \n",
      "      \"name\": \"hybridsequential0_dense0_fwd\", \n",
      "      \"attrs\": {\n",
      "        \"flatten\": \"True\", \n",
      "        \"no_bias\": \"False\", \n",
      "        \"num_hidden\": \"256\"\n",
      "      }, \n",
      "      \"inputs\": [[0, 0, 0], [1, 0, 0], [2, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"Activation\", \n",
      "      \"name\": \"hybridsequential0_dense0_relu_fwd\", \n",
      "      \"attrs\": {\"act_type\": \"relu\"}, \n",
      "      \"inputs\": [[3, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense1_weight\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(128, 0)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense1_bias\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__init__\": \"zeros\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(128,)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"FullyConnected\", \n",
      "      \"name\": \"hybridsequential0_dense1_fwd\", \n",
      "      \"attrs\": {\n",
      "        \"flatten\": \"True\", \n",
      "        \"no_bias\": \"False\", \n",
      "        \"num_hidden\": \"128\"\n",
      "      }, \n",
      "      \"inputs\": [[4, 0, 0], [5, 0, 0], [6, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"Activation\", \n",
      "      \"name\": \"hybridsequential0_dense1_relu_fwd\", \n",
      "      \"attrs\": {\"act_type\": \"relu\"}, \n",
      "      \"inputs\": [[7, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense2_weight\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(2, 0)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"hybridsequential0_dense2_bias\", \n",
      "      \"attrs\": {\n",
      "        \"__dtype__\": \"0\", \n",
      "        \"__init__\": \"zeros\", \n",
      "        \"__lr_mult__\": \"1.0\", \n",
      "        \"__shape__\": \"(2,)\", \n",
      "        \"__wd_mult__\": \"1.0\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"FullyConnected\", \n",
      "      \"name\": \"hybridsequential0_dense2_fwd\", \n",
      "      \"attrs\": {\n",
      "        \"flatten\": \"True\", \n",
      "        \"no_bias\": \"False\", \n",
      "        \"num_hidden\": \"2\"\n",
      "      }, \n",
      "      \"inputs\": [[8, 0, 0], [9, 0, 0], [10, 0, 0]]\n",
      "    }\n",
      "  ], \n",
      "  \"arg_nodes\": [0, 1, 2, 5, 6, 9, 10], \n",
      "  \"node_row_ptr\": [\n",
      "    0, \n",
      "    1, \n",
      "    2, \n",
      "    3, \n",
      "    4, \n",
      "    5, \n",
      "    6, \n",
      "    7, \n",
      "    8, \n",
      "    9, \n",
      "    10, \n",
      "    11, \n",
      "    12\n",
      "  ], \n",
      "  \"heads\": [[11, 0, 0]], \n",
      "  \"attrs\": {\"mxnet_version\": [\"int\", 10200]}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = mx.sym.var('data')\n",
    "y = net(x)\n",
    "print(y.tojson())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，对于`HybridBlock`的模块，既可以把NDArray作为输入，也可以把`Symbol`对象作为输入。当以`Symbol`作为输出时，它的结果就是一个`Symbol`对象。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
