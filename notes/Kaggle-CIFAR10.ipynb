{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Gluon对CIFAR-10数据集进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10数据集\n",
    "\n",
    "[CIFAR-10数据集](http://www.cs.toronto.edu/~kriz/cifar.html)是一个公开的目标识别的数据集，它的训练数据集一共包含了6万张图片，但图片的尺寸都比较小，为32×32的彩色图像。一共包含10类目标，每类目标包含6000张图片。测试数据集一共有30万张，其中1万张用来计分，但为了防止人工标测试集，里面另加了29万张不计分的图片。\n",
    "\n",
    "它是由 Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton收集整理，[相关论文](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T07:28:41.223540Z",
     "start_time": "2018-03-20T07:28:41.193288Z"
    }
   },
   "source": [
    "Kaggle上下载下来的数据集一共有3个文件\n",
    "- train.7z : 解压后为train文件夹里一共包含了5万张图片，命名规则是[1-50000].png\n",
    "- test.7z：解压后为test文件夹里一共包含了30万张图片，命名规则是[1-300000].png\n",
    "- trainLabels.csv：对应于train文件夹中每张图片的label，一共5万行，每行的格式为：id,label_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先呢，我们需要对数据集进行一定的整理，方便MXNet的数据读取脚本进行处理。整理后同一类的图片将出现在同一个文件夹下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：获取所有训练数据的标签，并建立对应的查找字典**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:50:23.399533Z",
     "start_time": "2018-03-22T04:50:23.351780Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_root_dir = '/home/yansheng/kaggle_dog'\n",
    "train_dir = 'train'\n",
    "label_file = 'labels.csv'\n",
    "test_dir = 'test'\n",
    "target_dir = 'train_valid_test'\n",
    "\n",
    "# 读取训练数据的标签，并把它保存在一个字典里\n",
    "\n",
    "with open(os.path.join(data_root_dir, label_file), 'r') as f:\n",
    "    lines = f.readlines()[1:] #第一行是头行，跳过去\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "labels = set(idx_label.values()) # 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:50:25.036446Z",
     "start_time": "2018-03-22T04:50:25.019317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boston_bull': 87, 'dingo': 80, 'pekinese': 75, 'bluetick': 85, 'golden_retriever': 67, 'bedlington_terrier': 89, 'borzoi': 75, 'basenji': 110, 'scottish_deerhound': 126, 'shetland_sheepdog': 76, 'walker_hound': 69, 'maltese_dog': 117, 'norfolk_terrier': 83, 'african_hunting_dog': 86, 'wire-haired_fox_terrier': 82, 'redbone': 72, 'lakeland_terrier': 99, 'boxer': 75, 'doberman': 74, 'otterhound': 69, 'standard_schnauzer': 72, 'irish_water_spaniel': 78, 'black-and-tan_coonhound': 77, 'cairn': 106, 'affenpinscher': 80, 'labrador_retriever': 84, 'ibizan_hound': 91, 'english_setter': 83, 'weimaraner': 85, 'giant_schnauzer': 69, 'groenendael': 82, 'dhole': 76, 'toy_poodle': 80, 'border_terrier': 91, 'tibetan_terrier': 107, 'norwegian_elkhound': 95, 'shih-tzu': 112, 'irish_terrier': 82, 'kuvasz': 71, 'german_shepherd': 69, 'greater_swiss_mountain_dog': 82, 'basset': 82, 'australian_terrier': 102, 'schipperke': 86, 'rhodesian_ridgeback': 88, 'irish_setter': 88, 'appenzeller': 78, 'bloodhound': 85, 'samoyed': 109, 'miniature_schnauzer': 78, 'brittany_spaniel': 73, 'kelpie': 86, 'papillon': 96, 'border_collie': 72, 'entlebucher': 115, 'collie': 87, 'malamute': 81, 'welsh_springer_spaniel': 79, 'chihuahua': 71, 'saluki': 99, 'pug': 94, 'malinois': 73, 'komondor': 67, 'airedale': 107, 'leonberg': 106, 'mexican_hairless': 80, 'bull_mastiff': 75, 'bernese_mountain_dog': 114, 'american_staffordshire_terrier': 74, 'lhasa': 90, 'cardigan': 76, 'italian_greyhound': 92, 'clumber': 80, 'scotch_terrier': 82, 'afghan_hound': 116, 'old_english_sheepdog': 87, 'saint_bernard': 84, 'miniature_pinscher': 102, 'eskimo_dog': 66, 'irish_wolfhound': 101, 'brabancon_griffon': 67, 'toy_terrier': 79, 'chow': 93, 'flat-coated_retriever': 72, 'norwich_terrier': 78, 'soft-coated_wheaten_terrier': 71, 'staffordshire_bullterrier': 79, 'english_foxhound': 86, 'gordon_setter': 81, 'siberian_husky': 95, 'newfoundland': 91, 'briard': 66, 'chesapeake_bay_retriever': 83, 'dandie_dinmont': 89, 'great_pyrenees': 111, 'beagle': 105, 'vizsla': 70, 'west_highland_white_terrier': 81, 'kerry_blue_terrier': 82, 'whippet': 95, 'sealyham_terrier': 88, 'standard_poodle': 79, 'keeshond': 81, 'japanese_spaniel': 105, 'miniature_poodle': 79, 'pomeranian': 111, 'curly-coated_retriever': 72, 'yorkshire_terrier': 82, 'pembroke': 92, 'great_dane': 75, 'blenheim_spaniel': 102, 'silky_terrier': 90, 'sussex_spaniel': 78, 'german_short-haired_pointer': 75, 'french_bulldog': 70, 'bouvier_des_flandres': 86, 'tibetan_mastiff': 69, 'english_springer': 75, 'cocker_spaniel': 74, 'rottweiler': 76}\n"
     ]
    }
   ],
   "source": [
    "org_label_count = {}\n",
    "for label in idx_label.values():\n",
    "    org_label_count[label] = org_label_count.get(label, 0) + 1\n",
    "print(org_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：遍历每一个训练数据，将其放到对应label的文件夹下**\n",
    "\n",
    "将train_dir里的所有图片，拷贝到target_dir下的三个目录中：\n",
    "\n",
    "- train_valid: 包括了完整的训练数据集\n",
    "- train: 只包含了用于训练的部分\n",
    "- valid: 只包含了用于验证的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:50:32.954484Z",
     "start_time": "2018-03-22T04:50:29.041874Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "valid_ratio = 0.2 # 验证集的比例\n",
    "label_count = dict() # 用户统计已经整理到每类文件夹中训练数据的数量，达到数量后，剩下的拷贝到验证集文件夹\n",
    "\n",
    "for train_file in os.listdir(os.path.join(data_root_dir, train_dir)):\n",
    "    idx = train_file.split('.')[0]\n",
    "    label = idx_label[idx]\n",
    "    mkdir_if_not_exist([data_root_dir, target_dir, 'train_valid', label])\n",
    "    shutil.copy(os.path.join(data_root_dir, train_dir, train_file),\n",
    "                    os.path.join(data_root_dir, target_dir, 'train_valid', label))\n",
    "    if label not in label_count or label_count[label] < org_label_count[label] * (1 - valid_ratio):\n",
    "            mkdir_if_not_exist([data_root_dir, target_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_root_dir, train_dir, train_file),\n",
    "                        os.path.join(data_root_dir, target_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "    else:\n",
    "        mkdir_if_not_exist([data_root_dir, target_dir, 'valid', label])\n",
    "        shutil.copy(os.path.join(data_root_dir, train_dir, train_file),\n",
    "                    os.path.join(data_root_dir, target_dir, 'valid', label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步：将测试数据集也按类别存放，因为没有对应的标签，所有都归类到unkown文件夹下**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:51:46.235016Z",
     "start_time": "2018-03-22T04:51:44.164805Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir_if_not_exist([data_root_dir, target_dir, 'test', 'unknown'])\n",
    "for test_file in os.listdir(os.path.join(data_root_dir, test_dir)):\n",
    "    shutil.copy(os.path.join(data_root_dir, test_dir, test_file),\n",
    "                os.path.join(data_root_dir, target_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Gluon读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义图像的预处理\n",
    "\n",
    "1. 将图片像素值转化为0-1的浮点数值\n",
    "2. 对图片进行翻转增强\n",
    "3. 对图片进行减均值，除方差处理。\n",
    "4. 将图片数据存储序由HWC转为CHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:51:54.695529Z",
     "start_time": "2018-03-22T04:51:54.584653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 96, 96)\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 96, 96),resize=0,\n",
    "                        rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                        mean=None,\n",
    "                        std=None,\n",
    "                        brightness=0.125, contrast=0.125,\n",
    "                        saturation=0, hue=0,\n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "# 测试时，无需对图像做标准化以外的增强数据处理。\n",
    "def transform_test(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 96, 96)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用Gluon的[`ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.datasets.ImageFolderDataset)类来读取整理后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:52:05.297202Z",
     "start_time": "2018-03-22T04:52:05.053318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_str = data_root_dir + '/' + target_dir + '/'\n",
    "batch_size = 128\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1,\n",
    "                                     transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1,\n",
    "                                     transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid',\n",
    "                                           flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1,\n",
    "                                     transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:52:21.453262Z",
     "start_time": "2018-03-22T04:52:21.199758Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # 模块1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # 模块2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # 模块3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # 模块4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # 模块5\n",
    "            net.add(nn.GlobalAvgPool2D())\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = len(labels)\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:52:23.461864Z",
     "start_time": "2018-03-22T04:52:23.397126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "resnet18 = get_net(ctx=mx.gpu())\n",
    "data = mx.sym.var('data')\n",
    "net_symbol = resnet18(data)\n",
    "#mx.viz.plot_network(net_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:52:24.375803Z",
     "start_time": "2018-03-22T04:52:24.349274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "     return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "def evaluate_accuracy(data_iter, net, ctx=mx.cpu()):\n",
    "    acc = 0\n",
    "    for data, label in data_iter:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "    return acc / len(data_iter)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T04:52:25.190471Z",
     "start_time": "2018-03-22T04:52:25.089054Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period, lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            pass\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc = evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T05:21:29.810820Z",
     "start_time": "2018-03-22T05:21:29.697236Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(data, net, ctx):\n",
    "    loss = 0.0\n",
    "    for feas, label in data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(feas.as_in_context(ctx))\n",
    "        cross_entropy = softmax_cross_entropy(output, label)\n",
    "        loss += nd.mean(cross_entropy).asscalar()\n",
    "    return loss / len(data)\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9,\n",
    "                                      'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = get_loss(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, Valid loss %f, \"\n",
    "                         % (epoch, train_loss / len(train_data), valid_loss))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, train_loss / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T05:21:35.947721Z",
     "start_time": "2018-03-22T05:21:35.935923Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lr_period = 10\n",
    "lr_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-22T05:21:06.913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 4.891469, Valid loss 4.882747, Time 00:01:05, lr 0.01\n",
      "Epoch 1. Train loss: 4.672906, Valid loss 4.636792, Time 00:01:25, lr 0.01\n",
      "Epoch 2. Train loss: 4.569214, Valid loss 4.733593, Time 00:01:25, lr 0.01\n",
      "Epoch 3. Train loss: 4.478176, Valid loss 4.587576, Time 00:01:25, lr 0.01\n",
      "Epoch 4. Train loss: 4.379323, Valid loss 4.568875, Time 00:01:27, lr 0.01\n",
      "Epoch 5. Train loss: 4.272317, Valid loss 4.987030, Time 00:01:26, lr 0.01\n",
      "Epoch 6. Train loss: 4.163936, Valid loss 5.074187, Time 00:01:26, lr 0.01\n",
      "Epoch 7. Train loss: 4.060266, Valid loss 4.602572, Time 00:01:25, lr 0.01\n",
      "Epoch 8. Train loss: 3.989079, Valid loss 4.582050, Time 00:01:26, lr 0.01\n",
      "Epoch 9. Train loss: 3.880605, Valid loss 4.808239, Time 00:01:26, lr 0.01\n",
      "Epoch 10. Train loss: 3.743272, Valid loss 4.061832, Time 00:01:25, lr 0.005\n",
      "Epoch 11. Train loss: 3.659518, Valid loss 4.002586, Time 00:01:24, lr 0.005\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate,\n",
    "      weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T17:08:58.553730Z",
     "start_time": "2018-03-20T15:57:09.959673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.855835, Train acc 0.322736, Time 00:01:06, lr 0.1\n",
      "Epoch 1. Loss: 1.368335, Train acc 0.499900, Time 00:01:07, lr 0.1\n",
      "Epoch 2. Loss: 1.121591, Train acc 0.596576, Time 00:01:06, lr 0.1\n",
      "Epoch 3. Loss: 0.921709, Train acc 0.675805, Time 00:01:06, lr 0.1\n",
      "Epoch 4. Loss: 0.765785, Train acc 0.730082, Time 00:01:06, lr 0.1\n",
      "Epoch 5. Loss: 0.643581, Train acc 0.775052, Time 00:01:06, lr 0.1\n",
      "Epoch 6. Loss: 0.562520, Train acc 0.804082, Time 00:01:05, lr 0.1\n",
      "Epoch 7. Loss: 0.507999, Train acc 0.822692, Time 00:01:07, lr 0.1\n",
      "Epoch 8. Loss: 0.450344, Train acc 0.844838, Time 00:01:07, lr 0.1\n",
      "Epoch 9. Loss: 0.419663, Train acc 0.855712, Time 00:01:06, lr 0.1\n",
      "Epoch 10. Loss: 0.317010, Train acc 0.890749, Time 00:01:08, lr 0.05\n",
      "Epoch 11. Loss: 0.270628, Train acc 0.908482, Time 00:01:05, lr 0.05\n",
      "Epoch 12. Loss: 0.254198, Train acc 0.913186, Time 00:01:07, lr 0.05\n",
      "Epoch 13. Loss: 0.235958, Train acc 0.917953, Time 00:01:06, lr 0.05\n",
      "Epoch 14. Loss: 0.226780, Train acc 0.921086, Time 00:01:07, lr 0.05\n",
      "Epoch 15. Loss: 0.215571, Train acc 0.926822, Time 00:01:05, lr 0.05\n",
      "Epoch 16. Loss: 0.207367, Train acc 0.927818, Time 00:01:05, lr 0.05\n",
      "Epoch 17. Loss: 0.193965, Train acc 0.933857, Time 00:01:04, lr 0.05\n",
      "Epoch 18. Loss: 0.192910, Train acc 0.932589, Time 00:01:06, lr 0.05\n",
      "Epoch 19. Loss: 0.188457, Train acc 0.935092, Time 00:01:05, lr 0.05\n",
      "Epoch 20. Loss: 0.105932, Train acc 0.965569, Time 00:01:06, lr 0.025\n",
      "Epoch 21. Loss: 0.065139, Train acc 0.980955, Time 00:01:04, lr 0.025\n",
      "Epoch 22. Loss: 0.050952, Train acc 0.986161, Time 00:01:04, lr 0.025\n",
      "Epoch 23. Loss: 0.044671, Train acc 0.987735, Time 00:01:06, lr 0.025\n",
      "Epoch 24. Loss: 0.039644, Train acc 0.989425, Time 00:01:07, lr 0.025\n",
      "Epoch 25. Loss: 0.035972, Train acc 0.990326, Time 00:01:06, lr 0.025\n",
      "Epoch 26. Loss: 0.030147, Train acc 0.992706, Time 00:01:05, lr 0.025\n",
      "Epoch 27. Loss: 0.031152, Train acc 0.991558, Time 00:01:05, lr 0.025\n",
      "Epoch 28. Loss: 0.038955, Train acc 0.988556, Time 00:01:06, lr 0.025\n",
      "Epoch 29. Loss: 0.042839, Train acc 0.986818, Time 00:01:06, lr 0.025\n",
      "Epoch 30. Loss: 0.023116, Train acc 0.994519, Time 00:01:07, lr 0.0125\n",
      "Epoch 31. Loss: 0.009475, Train acc 0.998784, Time 00:01:05, lr 0.0125\n",
      "Epoch 32. Loss: 0.007238, Train acc 0.999283, Time 00:01:04, lr 0.0125\n",
      "Epoch 33. Loss: 0.005578, Train acc 0.999661, Time 00:01:05, lr 0.0125\n",
      "Epoch 34. Loss: 0.004359, Train acc 0.999860, Time 00:01:07, lr 0.0125\n",
      "Epoch 35. Loss: 0.003648, Train acc 0.999940, Time 00:01:06, lr 0.0125\n",
      "Epoch 36. Loss: 0.003063, Train acc 0.999980, Time 00:01:04, lr 0.0125\n",
      "Epoch 37. Loss: 0.002955, Train acc 0.999980, Time 00:01:06, lr 0.0125\n",
      "Epoch 38. Loss: 0.002921, Train acc 0.999980, Time 00:01:05, lr 0.0125\n",
      "Epoch 39. Loss: 0.002812, Train acc 1.000000, Time 00:01:06, lr 0.0125\n",
      "Epoch 40. Loss: 0.002677, Train acc 1.000000, Time 00:01:07, lr 0.00625\n",
      "Epoch 41. Loss: 0.002579, Train acc 1.000000, Time 00:01:04, lr 0.00625\n",
      "Epoch 42. Loss: 0.002601, Train acc 0.999936, Time 00:01:06, lr 0.00625\n",
      "Epoch 43. Loss: 0.002535, Train acc 1.000000, Time 00:01:06, lr 0.00625\n",
      "Epoch 44. Loss: 0.002469, Train acc 1.000000, Time 00:01:06, lr 0.00625\n",
      "Epoch 45. Loss: 0.002636, Train acc 1.000000, Time 00:01:06, lr 0.00625\n",
      "Epoch 46. Loss: 0.002460, Train acc 1.000000, Time 00:01:07, lr 0.00625\n",
      "Epoch 47. Loss: 0.002476, Train acc 1.000000, Time 00:01:06, lr 0.00625\n",
      "Epoch 48. Loss: 0.002588, Train acc 1.000000, Time 00:01:05, lr 0.00625\n",
      "Epoch 49. Loss: 0.002464, Train acc 1.000000, Time 00:01:06, lr 0.00625\n",
      "Epoch 50. Loss: 0.002510, Train acc 1.000000, Time 00:01:06, lr 0.003125\n",
      "Epoch 51. Loss: 0.002471, Train acc 1.000000, Time 00:01:07, lr 0.003125\n",
      "Epoch 52. Loss: 0.002465, Train acc 1.000000, Time 00:01:06, lr 0.003125\n",
      "Epoch 53. Loss: 0.002479, Train acc 1.000000, Time 00:01:05, lr 0.003125\n",
      "Epoch 54. Loss: 0.002420, Train acc 1.000000, Time 00:01:06, lr 0.003125\n",
      "Epoch 55. Loss: 0.002438, Train acc 1.000000, Time 00:01:07, lr 0.003125\n",
      "Epoch 56. Loss: 0.002425, Train acc 1.000000, Time 00:01:05, lr 0.003125\n",
      "Epoch 57. Loss: 0.002452, Train acc 1.000000, Time 00:01:05, lr 0.003125\n",
      "Epoch 58. Loss: 0.002395, Train acc 1.000000, Time 00:01:05, lr 0.003125\n",
      "Epoch 59. Loss: 0.002416, Train acc 1.000000, Time 00:01:07, lr 0.003125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate,\n",
    "      weight_decay, ctx, lr_period, lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_data:\n",
    "    output = net(data.as_in_context(ctx))\n",
    "    preds.extend(output.argmax(axis=1).astype(int).asnumpy())\n",
    "\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key = lambda x:str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
